---
title: "Exploratory Data Analysis"
author: Giovanni Saraceno
output:
  pdf_document:
    keep_tex: true
    toc: true
  html_document:
    df_print: paged
    toc: true
date: ''
bibliography: ../notes.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Differently from **inferential statistics** (it will be addressed in the following sections) that deals with uncertainties in estimates and inferences about one or more populations, **Exploratory Data Analysis** aims to reveal interesting patterns and help to prepare the data in the best way for the following analyses. \
It can be roughly summarized in three big parts:

1. *Structure and summary of data*: To check the type of variables in the data set and compute location indexes (e.g., mean, median), variability indexes (e.g., variance) of the variables of interest
2. *Exploratory plots*: Histograms, box plots, bar plots, correlogram or scatter plots (e.g., skeweness, outliers)
3. *Preprocessing step*: Are there any NAs? Missing values? Integer variables to be converted in factors?

We will show the code using classical functions in R base and more recent `tidyverse` and `ggplot2`. 
```{r}
library(tidyverse)
library(dplyr)
```

We consider the data set `peso.Rdata` defined in the Introduction notes.
```{r}
dati <- read.table("../Introduction/peso.txt", sep=";") 
```

```{r}
str(dati)
attach(dati)
```

Generally, a data set contains information about a random sample. For each observation (rows), characteristics, also called variables (data set columns), are recorded. This means that the data represent realizations of one or more measurements performed on a sample of individuals.

Variables can be classified as:

*Qualitative (categorical)*: These express characteristics that cannot be measured numerically but allow assigning a sample observation to a category or group. For example, the variables X and Y are categorical. If there is no intrinsic order, the variables are called nominal, whereas if the values can be ordered, they are ordinal.
*Quantitative (numerical)*: The measurements are expressed as numbers. Numerical data can be continuous, like the variable W representing weight, or discrete, like the variable Z, representing the number of dependents.

## Discrete Variables

Discrete variables can take on a finite number of values. Both qualitative and discrete numerical variables fall into this category. In these cases, it is useful to examine the frequency distribution of all its values. In R, the `table()` function is used:
```{r}
table(X)
table(Y)
table(Z)
```
The same result can be obtained with the `summary()` function:
```{r}
summary(X)
```
Alternatively, we can calculate relative frequencies by dividing by the total number of observations:
```{r}
table(X) / length(X)
table(Y) / length(Y)
table(Z) / length(Z)
```
The frequency table can be represented using a bar plot:
```{r}
plot(table(X) / length(X))
```

If there is an intrinsic order, cumulative frequencies can be calculated and visualized:
```{r}
Y <- ordered(Y, levels = c("A", "O", "S", "L"))
cumsum(table(Y) / length(Y))
plot(ecdf(Y))
```

For numerical variables:
```{r}
cumsum(table(Z) / length(Z))
plot(ecdf(Z))
```

If we try the same approach for the continuous variable W:
```{r}
table(W)
```
It provides no useful information because all values are different, reflecting that W is described by a continuous random variable.

## Continuous Variables

Letâ€™s now explore descriptive statistics for continuous numerical variables.

Consider the following example: in a factory assembling devices, three different production line organizations are tested over three days: the current organization, old, and two new ones, new1 and new2. The productivity, measured as the number of completed operations, is recorded for 288 workers and stored in the data set organization.
```{r}
organization <- read.table("organizzazione.txt", sep=",")
```

```{r}
str(organization)
attach(organization)
```
Start by studying the variable old. Basic statistics include mean, standard deviation, variance, median, minimum, and maximum:
```{r}
mean(old)
sd(old)
var(old)
median(old)
range(old)
```
Empirical quantiles can be obtained with the `quantile()` function:
```{r}
quantile(old)
quantile(old, seq(0, 1, 0.1))  # Deciles
```
A summary of these key statistics can be obtained using the `summary()` function:
```{r}
summary(old)
summary(organization)
```
Frequency distributions of continuous numerical variables can be visualized with histograms:
```{r}
hist(old)
hist(old, freq = FALSE)  # Density
hist(old, freq = FALSE, breaks = 20)  # Custom intervals
```

The Freedman-Diaconis method for choosing interval widths:
```{r}
hist(old, freq = FALSE, breaks = "FD")
```

Kernel density estimation provides a continuous approximation of the empirical probability density:
```{r}
plot(density(old))
```

To compare with a known probability density (e.g., normal distribution):
```{r}
hist(old, freq = FALSE, breaks = seq(665, 740, 5), ylim = c(0, 0.04))
curve(dnorm(x, mean = mean(old), sd = sd(old)), col = "red", add = TRUE)
```

The empirical cumulative distribution function (CDF):
```{r}
plot(ecdf(old))
curve(pnorm(x, mean = mean(old), sd = sd(old)), add = TRUE, col = "red")
```

A QQ-plot compares the data against theoretical quantiles of a normal distribution:
```{r}
qqnorm(old)
qqline(old, col = "red")
```

Finally, boxplots summarize distributions visually:
```{r}
boxplot(old)
```

The three horizontal lines that make up the box in a **boxplot** represent the quartile values (from bottom to top: Q1, median, and Q3). The outer lines (the "whiskers") indicate the minimum and maximum values within a distance of \( 1.5 \times \text{IQ} \) from the nearest quartile, where \( \text{IQ} \) is the interquartile range. Any observation outside this range is considered "extreme," or an **outlier**, and is represented separately. 
This type of representation can be particularly useful when comparing two or more distributions.

The primary role of the statistics calculated so far is to characterize **location**, such as the mean and median, and **scale**, such as the standard deviation and interquartile range. To these measures, we can add **skewness** and **kurtosis**.

#### Skewness

**Skewness** is a measure of symmetry, or more precisely, the lack of symmetry in a distribution. A distribution is symmetric if it is evenly distributed around its central point. For example, the standard Gaussian distribution is symmetric around the mean \( \mu = 0 \).

Given a sample \( (Y_1, \ldots, Y_N) \), skewness can be calculated as:

\[
g_1 = \frac{\sum_{i=1}^N (Y_i - \bar{Y})^3 / N}{s^3}
\]

where \( \bar{Y} \) is the sample mean, \( s \) is the standard deviation, and \( N \) is the number of observations. Note that here \( s \) is calculated by dividing by \( N \) rather than \( N-1 \). This formula is known as the Fisher-Pearson coefficient of skewness. Some software also implements an adjusted version:

\[
G_1 = \frac{\sqrt{N(N-1)}}{N-2} \cdot \frac{\sum_{i=1}^N (Y_i - \bar{Y})^3 / N}{s^3}
\]

The skewness of a normal distribution is 0, as is the case for any symmetric distribution. Negative skewness indicates that the observations are more concentrated on the left, while positive skewness indicates a greater concentration on the right, with the right tail being longer than the left.

#### Kurtosis

**Kurtosis** compares the tails of a data distribution to those of a normal distribution, providing information on whether the distribution has heavier or lighter tails. The formula for kurtosis is:

\[
k_1 = \frac{\sum_{i=1}^N (Y_i - \bar{Y})^4 / N}{s^4}
\]

where \( \bar{Y} \) and \( s \) are the sample mean and standard deviation, and \( N \) is the number of observations. Again, \( s \) is calculated by dividing by \( N \). The kurtosis of the standard normal distribution is 3. Therefore, a more commonly used definition is:

\[
k_2 = \frac{\sum_{i=1}^N (Y_i - \bar{Y})^4 / N}{s^4} - 3
\]

This adjustment makes the kurtosis of the standard normal distribution equal to 0. Positive values (\textit{heavy-tailed}) indicate that the distribution has larger tails than a normal distribution, while negative values (\textit{light-tailed}) indicate smaller tails.

## Pre-processing

In practice, this step is generally very important and the most time-consuming. This step includes: 
- formatting of variables type, and consistent entrie; 
- dealing with *missing values*;
- dealing with *outliers*.


### What is an outlier?

An *outlier* is a value or an observation that is distant from other observations, that is to say, a data point that differs significantly from other data points. Alternatively, outliers can be defined as observations not following the *assumed model*, that is values that deviate so much from other observations one might suppose a different underlying sampling mechanism.

#### How to detect outliers R?

- Descriptive statistics
- Histogram
- Boxplots
- Percentiles

Outlier detection should be an important step during pre-processing, that is the identification of observations which "behave" differently with respect the majority of the data. \
Sometimes outliers may be due to some errors (measurement, reporting, ...) and then they can be discarded, substitued with `NA` (and then apply a method able to handle `NA`), or substituted with an *estimate* of their value. \
However, there is the possibility that these "anomalous" observations underline a specific pattern in the data, and then should be included and considered during the statistical analysis. 

### What is a missing value?

A missing value is one whose value is unknown. Missing values are represented in R by the `NA` (not available) symbol. `NA` is a special value whose properties are different from other values. \
Missing values can be informative, understanding if the missing is due to some reporting error or underline some specific pattern. One possible option during pre-processing would be removing the entire observations (rows) with at least one `NA`, or entire columns, if `NA`'s occur only for specific variables. \
**Remark**: In both cases we lose information, removing all the related available data. \
This may be a problem when our sample has few observations. An alternative option is to consider methods, from the literature, specifically designed for handling `NA`.

## Example: the Penguins data set

In this example, we consider the exploratory data analysis tools seen before using the example `penguins` data set available in the `palmerpenguins` package.
```{r}
library(palmerpenguins)
data("penguins")
```
Let's start displaying the structure and summary of the data set.
```{r}
str(penguins)
summary(penguins)
```
There are some missing values in the data set. As first step we can identfy the observations which contain NA values.
```{r}
penguins %>% filter(is.na(sex))
```
There are 2 observations with missing sex, bill length, bill depth, flipper length and body mass. We can remove them. \
There are additional 9 observations with missing value only in the sex variable. These observations could be maintained for the analysis that don't include sex. However, for this example we will remove all 11 rows. 
```{r}
peng_data <- penguins %>% filter(!is.na(sex))
summary(peng_data)
```
We can explore the variables singularly. 

For the discrete variables, we can display the frequencies
```{r}
p1 <- ggplot(data = peng_data, mapping = aes(x = species)) + 
  geom_bar(fill =  "green")

p2 <- ggplot(data = peng_data, mapping = aes(x = island)) + 
  geom_bar(fill= "blue")

p3 <- ggplot(data = peng_data, mapping = aes(x = sex)) + 
  geom_bar( fill= "purple")

p4 <- ggplot(data = peng_data, mapping = aes(x = year)) + 
  geom_bar(fill= "orange")

gridExtra::grid.arrange(p1,p2,p3,p4, nrow=2)
```

or we can combine information 
```{r}
p1 <- ggplot(data = peng_data, mapping = aes(x = species, fill =  island)) + 
  geom_bar()

p2 <- ggplot(data = peng_data, mapping = aes(x = island, fill= sex)) + 
  geom_bar()

p3 <- ggplot(data = peng_data, mapping = aes(x = species, fill= sex)) + 
  geom_bar()

gridExtra::grid.arrange(p2,p3,p1, nrow=2)
```

Equivalently we can display the relative frequencies. \
For the continuous variables, we can explore the single distributions using the histograms
```{r}
p1 <- ggplot(data = peng_data, mapping = aes(x = bill_length_mm)) + 
  geom_histogram(fill =  "green", bins = 40)

p2 <- ggplot(data = peng_data, mapping = aes(x = bill_depth_mm)) + 
  geom_histogram(fill= "blue", bins = 40)

p3 <- ggplot(data = peng_data, mapping = aes(x = flipper_length_mm)) + 
  geom_histogram( fill= "purple", bins = 40)

p4 <- ggplot(data = peng_data, mapping = aes(x = body_mass_g)) + 
  geom_histogram(fill= "orange", bins = 40)

gridExtra::grid.arrange(p1,p2,p3,p4, nrow=2)
```

The histograms show multimodal distributions for the considered variables. We can have additional information by considering the `species`
```{r}
p1 <- ggplot(data = peng_data, mapping = aes(x = bill_length_mm, color=species, fill =  species)) + 
  geom_histogram( bins = 40)

p2 <- ggplot(data = peng_data, mapping = aes(x = bill_depth_mm, fill =  species)) + 
  geom_histogram(bins = 40)

p3 <- ggplot(data = peng_data, mapping = aes(x = flipper_length_mm, color=species, fill =  species)) + 
  geom_histogram( bins = 40)

p4 <- ggplot(data = peng_data, mapping = aes(x = body_mass_g, color=species, fill =  species)) + 
  geom_histogram(bins = 40)

gridExtra::grid.arrange(p1,p2,p3,p4, nrow=2)
```

Alternatively we can show the estimated density
```{r}
ggplot(data = peng_data, aes(x = bill_length_mm, color = species)) + 
  geom_density(size=1) +
  geom_histogram(aes(y = after_stat(density),fill =  species), bins = 40, alpha=0.3) +
  theme_classic()
```   

The distribution of observations from a continuous random variable can be also displayed using the boxplot
```{r}
ggplot(data = peng_data, aes(y = bill_length_mm, fill = species)) + 
  geom_boxplot() +
  theme_classic()
```   

We can study the relationship between pairs of continuous variables using the scatter-plot
```{r}
p1 <- ggplot(peng_data, aes(x=bill_length_mm, y=bill_depth_mm)) +
  geom_point()
p2 <- ggplot(peng_data, aes(x=bill_length_mm, y=flipper_length_mm)) +
  geom_point()
p3 <- ggplot(peng_data, aes(x=bill_length_mm, y=body_mass_g)) +
  geom_point()
p4 <- ggplot(peng_data, aes(x=bill_depth_mm, y=flipper_length_mm)) +
  geom_point()
p5 <- ggplot(peng_data, aes(x=bill_depth_mm, y=body_mass_g)) +
  geom_point()
p6 <- ggplot(peng_data, aes(x=flipper_length_mm, y=body_mass_g)) +
  geom_point()
gridExtra::grid.arrange(p1, p2, p3,p4, p5, p6, nrow=2)
```

We can see that data points show different patterns. We can color data points by the species
```{r}
p1 <- ggplot(peng_data, aes(x=bill_length_mm, y=bill_depth_mm, color=species)) +
  geom_point()
p2 <- ggplot(peng_data, aes(x=bill_length_mm, y=flipper_length_mm, color=species)) +
  geom_point()
p3 <- ggplot(peng_data, aes(x=bill_length_mm, y=body_mass_g, color=species)) +
  geom_point()
p4 <- ggplot(peng_data, aes(x=bill_depth_mm, y=flipper_length_mm, color=species)) +
  geom_point()
p5 <- ggplot(peng_data, aes(x=bill_depth_mm, y=body_mass_g, color=species)) +
  geom_point()
p6 <- ggplot(peng_data, aes(x=flipper_length_mm, y=body_mass_g, color=species)) +
  geom_point()
gridExtra::grid.arrange(p1, p2, p3,p4, p5, p6, nrow=3)
```

We could add also the island and/or sex information to the same plot
```{r}
ggplot(peng_data, aes(x=bill_length_mm, y=bill_depth_mm, color=species)) +
  geom_point(aes(shape=sex))+
  facet_wrap(.~island)
```

**Remark**: Plots with several levels of information can be very useful when exploring the data set, since they allow to have a general view of the data set and the relations among variables. On the other hand, when showing results they could appear unclear or overwhelming. Think carefully on the aspect of the data set that you want to explore or show, then create specific plots. 

Finally, we consider the correlation among variables
```{r}
corr_matrix <- round(cor(peng_data[,3:6]), 2)
corr_matrix

library(ggcorrplot)                                             
ggcorrplot(corr_matrix, 
           type = "upper", 
           lab = T, 
           lab_size = 7, 
           outline.col = "white", 
           colors = c("tomato2", "white", "springgreen3"), 
           title = "", 
           ggtheme = theme_gray, 
           pch.cex = 30, 
           tl.cex = 20)
```

A fast way for visualizing all these plots at once is provided by the function `ggpairs` in the `GGally` package
```{r, message=FALSE,warning=FALSE}
library(GGally)
ggpairs(peng_data[,c(1,3:6)], aes(color=species))
```

## Exercises

### Exercise 6
Write a function named `kurtosis` to calculate the kurtosis indices \( k_1 \) and \( k_2 \) given in the formulas:

\[
k_1 = \frac{\sum_{i=1}^N (Y_i - \bar{Y})^4 / N}{s^4}
\]

\[
k_2 = \frac{\sum_{i=1}^N (Y_i - \bar{Y})^4 / N}{s^4} - 3
\]

Write a function named `skewness` is a similar way. 

### Exercise 7
The data file `fondi.txt` contains the returns of 30 funds categorized into two types labeled `A` and `B`. Analyze the two variables separately. Specifically:

1. Create a table with the minimum, \( Q_1 \), mean, median, \( Q_3 \), maximum, standard deviation, and interquartile range for both variables, rounded to two decimal places.
2. Plot the density and empirical cumulative distribution functions for each variable separately.
3. Compute skewness and kurtosis indices.
4. Compare the two variables using boxplots and empirical cumulative distribution functions after standardizing the data.

Comment on the results.
